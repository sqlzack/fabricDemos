{"cells":[{"cell_type":"markdown","id":"7e52ca43-1107-4d7b-a4d2-c42266d05801","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set up Variables to be used throughout\n","Variable Descriptions Below\n","- relativePath - Where your NYC Taxi FOIL 2013 Files are located. The Lakehouse path will mose likely start with Files/ and then you can define your path from there.\n","- lakehouseName - The name of the Lakehouse in your Workspace.\n","- outputTable - The name of the table that will be created after loading csvs"]},{"cell_type":"code","execution_count":null,"id":"9b1be95e-f445-487b-a811-c2129c41ccd7","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["relativePath = ''\n","lakehouseName = ''\n","outputTable = \"\""]},{"cell_type":"markdown","id":"1b7b23f4-9e10-4817-b9ec-51321d422c1d","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Define a Function to Update Control Table\n","- The control table used in this process will follow the processing state of a file as it flows into the Lakehouse.\n","- This merge will happen in multiple parts of the process so I abstracted it into a function. "]},{"cell_type":"code","execution_count":null,"id":"ff6f718c-935e-435d-b2a5-0e2b8c518cec","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def updateFileLog(updateFileName, updatePath,updateStatus):\n","    staticQuery = f\"\"\"\n","    SELECT '{updateFileName}' as fileName\n","            ,'{updatePath}' as relativePath\n","            ,'{updateStatus}' as  loadStatus\n","            ,to_utc_timestamp(current_date(),current_timezone()) as loadStartDate\n","            ,NULL as loadEndDate\n","    \"\"\"\n","\n","    spark.sql(staticQuery).createOrReplaceTempView(\"v_fileLog_inProcess\")\n","\n","    mergeQuery = f\"\"\"\n","    MERGE INTO control_filelog dest\n","    USING v_fileLog_inProcess src\n","    ON  dest.fileName = src.fileName\n","    AND dest.relativePath = src.relativePath\n","    WHEN MATCHED THEN\n","        UPDATE\n","        SET loadStatus = src.loadStatus,\n","            loadEndDate = to_utc_timestamp(current_date(),current_timezone())\n","    WHEN NOT MATCHED THEN \n","        INSERT *\n","    \"\"\"\n","\n","    dfMergeResults = spark.sql(mergeQuery)"]},{"cell_type":"markdown","id":"9ee2c673-c8f1-40e4-b747-97299c573df3","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Collect all the files in the directory into a List for Evaluating/Processing\n","- Reference mssparkutils [here](https://learn.microsoft.com/en-us/fabric/data-engineering/microsoft-spark-utilities) for a list of useful operations that can be performed.\n","- In this case I only want to evaluate files so I'm iterating through the list that is produced from the mssparkutils ls command and taking only the files."]},{"cell_type":"code","execution_count":null,"id":"25e614c9-8c36-41c1-adf7-15eca8dda253","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["listDirContents = mssparkutils.fs.ls(relativePath)\n","listFiles = []\n","\n","for contents in listDirContents:\n","    if not contents.isDir:\n","        listFiles.append(contents.name)"]},{"cell_type":"markdown","id":"6369e348-0afa-4022-8712-63d087fb58c2","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Load New Files Into Bronze Table"]},{"cell_type":"code","execution_count":null,"id":"7c429261-4574-4740-a661-220d47fceb00","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","from pyspark.sql.functions import lit\n","\n","for fileName in listFiles:\n","    # check for files in log \n","    checkSQL = spark.sql(f\"SELECT * FROM control_filelog WHERE fileName = '{fileName}' and relativePath = '{relativePath}'\")\n","    ctFilesInLog = checkSQL.count()\n","\n","    # if files not in log then process them\n","    if ctFilesInLog == 0:\n","        # 1 - Add to log\n","        updateFileLog(fileName,relativePath,'inprocess')\n","\n","        try:\n","            # 2 - Load file to dataframe and remove spaces from column names\n","            inprocessDf = spark.read.option(\"header\",True).csv(f\"{relativePath}/{fileName}\")\n","            inprocessDfCleanColNames = inprocessDf.select([F.col(col).alias(col.replace(' ', '_')) for col in inprocessDf.columns])\n","            inprocessDfAddFilename = inprocessDfCleanColNames.withColumn(\"filename\",lit(fileName))\n","\n","            # 3 - Determine if output table exists. If it does, merge data. If not, create table from first file being processed.\n","            checkTableExists = spark.catalog.tableExists(outputTable)\n","\n","            if checkTableExists:\n","                inprocessDfAddFilename.write.mode(\"append\").format(\"delta\").save(f\"Tables/{outputTable}\")\n","            else:  \n","                inprocessDfAddFilename.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{outputTable}\")\n","\n","            #4 Update status and and enddate in control table and move files to appropriate directory\n","            updateFileLog(fileName,relativePath,'complete')\n","            mssparkutils.fs.mkdirs(f\"{relativePath}/Archive/\")\n","            mssparkutils.fs.mv(f\"{relativePath}/{fileName}\",f\"{relativePath}/Archive/{fileName}\")\n","        except:\n","            updateFileLog(fileName,relativePath,\"error\")\n","            mssparkutils.fs.mkdirs(f\"{relativePath}/Error/\")\n","            mssparkutils.fs.mv(f\"{relativePath}/{fileName}\",f\"{relativePath}/Error/{fileName}\")"]},{"cell_type":"markdown","id":"bb20555f-2589-4ab7-b9fe-4320cd29da63","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Pre-requisite control table DDL"]},{"cell_type":"code","execution_count":null,"id":"48eacac6-5217-4fe2-b8bf-f2d71e3d7deb","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CREATE TABLE control_fileLog \n","# (\n","#     fileName        string,\n","#     relativePath    string,\n","#     loadStatus      string,\n","#     loadStartDate   timestamp,\n","#     loadEndDate     timestamp\n","# )\n","# USING delta\n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{"532bb897-a832-4e70-a658-1633e451bc0a":{"persist_state":{"view":{"chartOptions":{"aggregationType":"count","binsNumber":10,"categoryFieldKeys":["0"],"chartType":"bar","isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"language":"scala","table":{"rows":[{"0":"trip_data_2.csv","1":"Files/tripdata","2":"Complete","3":"2023-07-19 00:00:00","4":"NULL","index":1}],"schema":[{"key":"0","name":"fileName","type":"string"},{"key":"1","name":"relativePath","type":"string"},{"key":"2","name":"loadStatus","type":"string"},{"key":"3","name":"loadStartDate","type":"timestamp"},{"key":"4","name":"loadEndDate","type":"void"}],"truncated":false}},"type":"Synapse.DataFrame"}},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"40899024-5cad-4d94-afe6-ba248cc36b84","default_lakehouse_name":"demoLakehouse","default_lakehouse_workspace_id":"5d577cd4-8b6d-4c9b-a4ec-e394748d1256","known_lakehouses":[{"id":"40899024-5cad-4d94-afe6-ba248cc36b84"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
