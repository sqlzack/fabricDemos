{"cells":[{"cell_type":"markdown","id":"8945e622-6d9d-4f9e-a98a-45cb10d7cc61","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set up connection information for KQL Database\n","\n","Information on variables below.\n","- kustoCluster - Copy the value found in the walkthrough [here](https://learn.microsoft.com/en-us/fabric/data-factory/connector-kusto-dataflows#copy-the-query-uri).\n","- kustoDb - The name of your KQL Database\n","- kustoTable - The KQL Database Table you'll be copying data into the Lakehouse from."]},{"cell_type":"code","execution_count":null,"id":"72e2a77a-f6a1-4dbe-9c83-88a231e2e35a","metadata":{},"outputs":[],"source":["kustoCluster = ''\n","kustoDb = ''\n","token = mssparkutils.credentials.getToken(kustoCluster)\n","kustoTable = \"\""]},{"cell_type":"markdown","id":"ade59f27-d296-439b-8b1a-b1d284966f18","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set up function to accept a KQL Query against the KQL Database\n","You will make multiple calls to KQL Database in this process and this function works in conjunction with the variables you have already set up to execute those queries."]},{"cell_type":"code","execution_count":null,"id":"9c81df92-6cb5-479c-8ddf-e32549f1de33","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def runKustoQuery(kustoQuery):\n","    dfKusotResult = spark.read \\\n","        .format(\"com.microsoft.kusto.spark.datasource\")\\\n","        .option(\"kustoCluster\", kustoCluster).option(\"kustoDatabase\", kustoDb) \\\n","        .option(\"kustoDatabase\", kustoDb) \\\n","        .option(\"accessToken\", token)  \\\n","        .option(\"kustoQuery\", kustoQuery) \\\n","        .load()\n","    \n","    return dfKusotResult"]},{"cell_type":"markdown","id":"224750e5-d554-45d2-9738-ade3c4c7b21a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Get a _cleaned_ week of data from KQL Database\n","This function accepts a week date and runs a query to clean the data in the KQL Database table before bringing it to the Silver Lakehouse table. This will serve as the base of the merge statement."]},{"cell_type":"code","execution_count":null,"id":"470f323d-240d-404d-9375-832afa95aa37","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def getWeekData(weekInProcess):\n","    weekDataQuery = f\"\"\"\n","    faredata_raw\n","    | where endofweek(todatetime(pickup_datetime)) == endofweek(todatetime(\"{weekInProcess}\"))\n","    | summarize fareAmount=sum(fare_amount),\n","                surcharge=sum(surcharge),\n","                mtaTax=sum(mta_tax),\n","                tipAmount=sum(tip_amount),\n","                tollsAmount=sum(tolls_amount),\n","                totalAmount=sum(total_amount),\n","                maxIngestDate=max(ingestion_time())\n","            by  medallion,\n","                hackLicense=hack_license,\n","                vendorId=vendor_id,\n","                pickupDatetime=pickup_datetime,\n","                paymentType=payment_type\n","    | extend ingestDate = ingestion_time()\n","    | extend rowKey = hash_sha256(strcat(medallion,hackLicense,vendorId,pickupDatetime,paymentType))\n","    \"\"\"\n","    \n","    dfWeekInProcess = runKustoQuery(weekDataQuery)\n","    dfWeekInProcess.createOrReplaceTempView(\"v_aggWeek\")"]},{"cell_type":"markdown","id":"a993f798-90f6-484b-98ab-c800eecc2d42","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set up Control table for Batching the stream data coming from KQL DB\n","The Fabric capacity I'm working on in this Demo is very small. Therefore I have to batch my streaming data into chunks that will not overtax the capacity. Since my data is reliable I know that each week constitutes ~2 Million rows which is acceptable for the downstream processing."]},{"cell_type":"code","execution_count":null,"id":"cf54ecbf-8693-4f13-b232-843d9369e961","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# This is the query that will ascertain whether I have ingested new data for the\n","# weeks of fare data that exist in KQL DB. \n","kqlIngestByWeek= \"\"\"\n","faredata_raw\n","| summarize maxIngestDate = max(ingestion_time()) by endOfWeekDate = endofweek(todatetime(pickup_datetime)) \n","\"\"\"\n","\n","# Run the Query using the function defined in a previous step and save it to a temporary view.\n","dfIngestByWeek = runKustoQuery(kqlIngestByWeek)\n","dfIngestByWeek.createOrReplaceTempView(\"v_ingestByWeek\")\n","\n","# Use the temporary view to merge records to the kustoLog Lakehouse Delta Table.\n","# If a new week is detected it is inserted. If a week already exists but has ingested new data it is updated.\n","kustoLogMergeQuery = \"\"\"\n","MERGE INTO control_kustoLog dest\n","USING v_ingestByWeek src\n","ON dest.endOfWeekDate = src.endOfWeekDate\n","WHEN MATCHED AND src.maxIngestDate > dest.maxIngestDate THEN\n","    UPDATE \n","    SET dest.maxIngestDate = src.maxIngestDate,\n","        dest.status = 'new'\n","WHEN NOT MATCHED THEN \n","    INSERT (maxIngestDate, endOfWeekDate, status)\n","    VALUES (src.maxIngestDate, src.endOfWeekDate, 'new')\n","\"\"\"\n","\n","dfLogMergeQueryResults = spark.sql(kustoLogMergeQuery)"]},{"cell_type":"markdown","id":"eca36308-4cf1-4b9f-b5d8-86ea460bfbf9","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Process New or Changed Weeks\n"]},{"cell_type":"code","execution_count":null,"id":"c1777445-f289-433f-b30f-fca152ca284e","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Determine which weeks are \"new\" from the control log. (These weeks may also have already existed but have newer data)\n","dfWeeksToProcess = spark.sql(\"SELECT * FROM control_kustolog WHERE status = 'new' order by endOfWeekDate asc limit 1\")\n","\n","# Collect the weeks for and iterate through each new week\n","for row in dfWeeksToProcess.rdd.collect():\n","\n","    # Bring the current week into a variable\n","    weekInProcess = row.endOfWeekDate\n","    \n","    # Run the KQL DB query needed to get the week's data. Output will be a temp view used in Merge\n","    getWeekData(weekInProcess)\n","\n","    # Construct the Merge query to be run in the next step.\n","    weekMergeQuery = \"\"\"\n","    MERGE INTO silver_faredata dest\n","    USING v_aggWeek src\n","    ON dest.rowKey = src.rowKey\n","    WHEN MATCHED AND to_timestamp(src.maxIngestDate) > dest.maxIngestDate THEN\n","        UPDATE\n","        SET fareAmount = src.fareAmount\n","    WHEN NOT MATCHED THEN \n","        INSERT *\n","    \"\"\"\n","\n","    # Merge the data and capture the outcome of that action in a Data Frame \n","    # To-do: Add try/catch handling and maybe some row count logging based on this output\n","    dfWeekMergeResults = spark.sql(weekMergeQuery)\n","\n","    # Update the Log to reflect that the batch has completed successfully\n","    updateLogQuery = f\"UPDATE control_kustolog SET status = 'completed' WHERE endofWeekDate = '{weekInProcess}'\"\n","\n","    dfUpdateLogResults = spark.sql(updateLogQuery)\n","\n","    # display(dfUpdateLogResults)\n"]},{"cell_type":"markdown","id":"9d3a33b9-5e26-4982-b65a-8316c52b6b08","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Pre-requisite Tables"]},{"cell_type":"code","execution_count":null,"id":"b8cbac62-6db6-4cf2-b15b-7ed7985e8e35","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CREATE TABLE IF NOT EXISTS silver_faredata\n","# (\n","#     medallion int,\n","#     hackLicense int,\n","#     vendorId string,\n","#     pickupDatetime timestamp,\n","#     paymentType string,\n","#     fareAmount decimal(38,5),\n","#     surcharge decimal(38,5),\n","#     mtaTax decimal(38,5),\n","#     tipAmount decimal(38,5),\n","#     tollsAmount decimal(38,5),\n","#     totalAmount decimal(38,5),\n","#     maxIngestDate TIMESTAMP,\n","#     rowKey string\n","# )\n","# USING DELTA\n","\n","# CREATE TABLE IF NOT EXISTS control_kustoLog\n","# (\n","#     endOfWeekDate timestamp,\n","#     maxIngestDate timestamp,\n","#     status  string\n","# )\n","# USING DELTA"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"40899024-5cad-4d94-afe6-ba248cc36b84","default_lakehouse_name":"demoLakehouse","default_lakehouse_workspace_id":"5d577cd4-8b6d-4c9b-a4ec-e394748d1256","known_lakehouses":[{"id":"40899024-5cad-4d94-afe6-ba248cc36b84"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
